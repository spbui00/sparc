from .core.base_method import BaseSACMethod
from .core.evaluator import Evaluator
from .core.signal_data import SignalDataWithGroundTruth
from typing import Dict, Any

class MethodTester:
    def __init__(self, data: SignalDataWithGroundTruth, methods: Dict[str, BaseSACMethod]):
        """
        Args:
            data: A SignalDataWithGroundTruth object containing the data to test.
            methods: A dictionary of artifact removal methods to test.
        """
        self.data = data
        self.methods = methods
        self.results: Dict[str, Any] = {}
        self.evaluator = Evaluator(sampling_rate=self.data.sampling_rate)

        # Set sampling rate for methods if not already set
        for method in self.methods.values():
            if method.sampling_rate is None:
                method.set_sampling_rate(self.data.sampling_rate)

    def run(self):
        """
        Runs the test by fitting and transforming the data with each method,
        then evaluates the results using SPARCEvaluator.
        """
        print(f"Running tests with {list(self.methods.keys())} methods.")
        
        cleaned_signals = {}
        for name, method in self.methods.items():
            print(f"Running method: {name}")
            method.fit(self.data.raw_data)
            cleaned_signals[name] = method.transform(self.data.raw_data)
            self.results[name] = self.evaluator.evaluate(
                ground_truth=self.data.ground_truth_spikes,
                original_mixed=self.data.raw_data,
                cleaned=cleaned_signals[name]
            )

    def get_results(self):
        """Returns the results of the tests."""
        return self.results

    def print_results(self):
        """Prints a summary of the results.
        Note: The main summary is already printed by the evaluator during the run.
        This provides a way to access it again.
        """
        if not self.results:
            print("No results to print. Run the tester first.")
            return

        print("\n=== Full Results Summary ===")
        for method_name, metrics in self.results.items():
            print(f"\n--- {method_name} ---")
            for metric_name, value in metrics.items():
                if isinstance(value, float):
                    print(f"  {metric_name}: {value:.4f}")

    def plot_results(self, channel=0):
        """Plot results for a single channel.
        Note: Comparison plots are already generated by the evaluator during the run.
        This provides a way to plot the time series data.
        """
        import matplotlib.pyplot as plt

        # In this new design, the cleaned signals are not stored in self.results by default
        # to save memory, as the evaluator handles the metrics. We regenerate them for plotting.
        cleaned_signals = {}
        print("Generating cleaned signals for plotting...")
        for name, method in self.methods.items():
            cleaned_signals[name] = method.transform(self.data.raw_data)

        if not cleaned_signals:
            print("No cleaned signals to plot.")
            return

        fig, axes = plt.subplots(len(cleaned_signals) + 1, 1, figsize=(15, 10), sharex=True)
        
        axes[0].plot(self.data.raw_data[:, channel], label='Original', color='black', alpha=0.5)
        axes[0].plot(self.data.ground_truth_spikes[:, channel], label='Ground Truth', color='blue', alpha=0.7)
        axes[0].set_title("Original Data")
        axes[0].legend()

        for i, (name, cleaned_data) in enumerate(cleaned_signals.items()):
            ax = axes[i+1]
            ax.plot(self.data.raw_data[:, channel], label='Original', color='black', alpha=0.2)
            ax.plot(self.data.ground_truth_spikes[:, channel], label='Ground Truth', color='blue', alpha=0.3)
            ax.plot(cleaned_data[:, channel], label=f'Cleaned with {name}', color='green')
            ax.set_title(f"Cleaned with {name}")
            ax.legend()

        plt.tight_layout()
        plt.show()
